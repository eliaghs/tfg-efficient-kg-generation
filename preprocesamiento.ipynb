{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pdfplumber**\n",
    "\n",
    "https://pypi.org/project/pdfplumber/\n",
    "\n",
    "\n",
    "Works best on machine-generated, rather than scanned, PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "path = r\"C:\\Users\\eliag\\OneDrive - Universidad Politécnica de Madrid\\CUATRI7\\TFG\\PREPROCESSING\\basketball_notes.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El texto ha sido guardado en 'salida.txt'.\n"
     ]
    }
   ],
   "source": [
    " # Nombre del archivo de salida\n",
    "archivo_salida = \"salida.txt\"\n",
    " \n",
    "# Abre el archivo de salida en modo escritura con codificación UTF-8\n",
    "with open(archivo_salida, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    # Abre el archivo PDF\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        # Itera sobre las páginas del PDF y escribe directamente en el archivo\n",
    "        for num_pagina, pagina in enumerate(pdf.pages, start=1):\n",
    "            texto_pagina = pagina.extract_text()\n",
    "            if texto_pagina:\n",
    "                archivo.write(f\"--- Página {num_pagina} ---\\n\")\n",
    "                archivo.write(texto_pagina + \"\\n\\n\")  # Añade salto de línea entre páginas\n",
    " \n",
    "print(f\"El texto ha sido guardado en '{archivo_salida}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salida.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivo_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolución de correferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocesamiento completado. Guardado en 'preprocesado.txt'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CoreferencesResolver:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\")  \n",
    "\n",
    "    def replace_references(self, doc):\n",
    "        \"\"\" Reemplaza menciones por su referencia principal. \"\"\"\n",
    "        token_mention_mapper = {}\n",
    "        resolved_text = \"\"\n",
    "\n",
    "        # Obtener los clusters de correferencia\n",
    "        clusters = [val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")]\n",
    "\n",
    "        for cluster in clusters:\n",
    "            main_mention = cluster[0]  # Referencia principal\n",
    "            main_text = main_mention.text\n",
    "\n",
    "            for mention in cluster[1:]:  # Omitir la principal\n",
    "                token_mention_mapper[mention[0].idx] = main_text + mention[0].whitespace_\n",
    "                for token in mention[1:]:\n",
    "                    token_mention_mapper[token.idx] = \"\"  # Eliminar menciones secundarias\n",
    "\n",
    "        # Construir el texto resuelto\n",
    "        for token in doc:\n",
    "            resolved_text += token_mention_mapper.get(token.idx, token.text + token.whitespace_)\n",
    "\n",
    "        return resolved_text\n",
    "\n",
    "    def process_text(self):\n",
    "        \"\"\" Procesa el texto: resuelve correferencias y segmenta en oraciones. \"\"\"\n",
    "        with open(self.input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "        resolved_text = self.replace_references(doc)\n",
    "\n",
    "        # Volver a procesar el texto corregido\n",
    "        doc = self.nlp(resolved_text)\n",
    "        sentences = [sent.text for sent in doc.sents]  # Segmentación en oraciones\n",
    "\n",
    "        # Guardar en archivo\n",
    "        with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in sentences:\n",
    "                f.write(sentence + \"\\n\")\n",
    "\n",
    "        print(f\"✅ Preprocesamiento completado. Guardado en '{self.output_file}'.\")\n",
    "\n",
    "# Ejecutar el preprocesamiento\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"salida.txt\"\n",
    "    output_file = \"preprocesado.txt\"\n",
    "\n",
    "    resolver = CoreferencesResolver(input_file, output_file)\n",
    "    resolver.process_text()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
